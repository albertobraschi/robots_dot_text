RobotsDotText
=============

Robots_dot_text allows you to create a robots.txt file dynamically using Ruby.
This gives you the option to create directives that are updated dynamically as your site's content changes.

...it's also a bit more fun and easier than having to remember the names of all of the various crawlers and the syntax for robots.txt files.

As a useful extra, you have the option to log every request made to /robots.txt in a separate log file called 
user_agents.log - Perfect for spider-spotters

The user_agents.log is written in csv(ish) format so you can change the extension and upload it to a database 
or spreadsheet of your choice.

All feedback welcome: dr_gavin@hotmail.com

==Setup Instructions

script/plugin install http://github.com/GavinM/robots_dot_text.git

remove <b>robots.txt</b> from your <em>/public</em> directory

create a controller for your robots with an action called index
	<tt>script/generate controller robots index</tt>

add a route in <b>routes.rb</b> to your robots index action.
	<tt>map.connect "robots.txt", :controller => "robots"</tt>

See the examples below for implementation.


==Examples:

===Simple example
class RobotsController < ActionController::Base
	def	index
	  respond_to do |format|
	    format.text do
				log_user_agent # adds the crawler's user_agent to user_agents.log
				@page_content = robots_dot_text do |rules|
					rules.comment "Tell all crawlers to keep out of these pages"
					rules.add :all, user_path("*"), admin_path, customers_path, log_path
					rules.br
					rules.sitemap sitemap_url
				end
				render :text => @page_content, :layout => false
			end
		end
	end
end

will render:
	# Tell all crawlers to keep out of these pages
	User-agent: *
	Disallow: user/*
	Disallow: /admin
	Disallow: /customers
	Disallow: /log

Sitemap: http://handyrailstips.com/sitemap.xml

===Complex Example

class RobotsController < ActionController::Base
	
	def	index
	  respond_to do |format|
	    format.txt do
        log_user_agent(:short, logger) # :short is the datetime format, logger specifies to use Rails.logger instead
        @page_content = robots_dot_text do |rules|
          rules.add :all
          rules.sitemap sitemap_url, google_news_sitemap_url
          rules.br
          rules.comment "Google ignores most directives so here are some rules for Google"
          rules.add [:google, :google_image, :google_mobile]
          rules.allow "/articles/*"
          rules.block articles_path
          rules.line_break
          rules.comment "These crawlers respect the Crawl-delay directive"
          rules.add [:yahoo, :msn, :cuil, :ask], private_path, admin_path
          rules.rate "1/500s"
          rules.delay 10
          rules.comment "Request robots only crawl between 2am and 8am. (Those are our quiet times)"
          rules.visit_time "0200", "0800"
        end
        render :text => @page_content, :layout => false
      end
    end
  end

end
</tt>
will render:

	User-agent: *
	Sitemap: http://handyrailstips.com/sitemap.xml
	Sitemap: http://handyrailstips.com/google_news_sitemap.xml

	# Google ignores most directives so here are some rules for Google
	User-agent: Googlebot
	User-agent: Googlebot-Image
	User-agent: Googlebot-Mobile
	Allow: /articles/show
	Disallow: /articles

	# These crawlers respect the Crawl-delay directive
	User-agent: Slurp
	User-agent: MSNBot
	User-agent: Twiceler
	User-agent: Teoma
	Disallow: /private/
	Disallow: /admin
	Request-rate: 1/500s
	Crawl-delay: 10
	# Request robots only crawl between 2am and 8am.
	# (Those are our quiet times)
	Visit-time: 0200-0800

Check out the RDocs - <em>robots_dot_text/doc/index.html</em>

For more info on crawler user-agents check out http://www.user-agents.org/

Copyright Â© 2009 Gavin Morrice, released under the MIT license